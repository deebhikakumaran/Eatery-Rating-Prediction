{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":111489,"databundleVersionId":13272433,"sourceType":"competition"}],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Importing Libraries","metadata":{}},{"cell_type":"code","source":"import numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.express as px\nimport re\nfrom scipy.stats import skew\nfrom datetime import datetime, timedelta\nimport nltk\nfrom wordcloud import STOPWORDS\nfrom wordcloud import WordCloud\nimport optuna\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom skopt import BayesSearchCV\nfrom skopt.space import Real, Integer, Categorical\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.impute import KNNImputer\nfrom sklearn.model_selection import GridSearchCV, StratifiedKFold, RandomizedSearchCV, cross_val_score\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.naive_bayes import MultinomialNB, GaussianNB\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.dummy import DummyClassifier\nfrom sklearn.metrics import accuracy_score, classification_report, precision_score, recall_score, f1_score, roc_auc_score, ConfusionMatrixDisplay\nimport warnings\nimport pickle\nwarnings.filterwarnings(\"ignore\")\n\n%matplotlib inline\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-01T12:19:38.785283Z","iopub.execute_input":"2025-09-01T12:19:38.785681Z","iopub.status.idle":"2025-09-01T12:19:38.797598Z","shell.execute_reply.started":"2025-09-01T12:19:38.785642Z","shell.execute_reply":"2025-09-01T12:19:38.796919Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/mlp-term-2-2025-kaggle-assignment-3/sample_submission.csv\n/kaggle/input/mlp-term-2-2025-kaggle-assignment-3/train.csv\n/kaggle/input/mlp-term-2-2025-kaggle-assignment-3/test.csv\n","output_type":"stream"}],"execution_count":2},{"cell_type":"markdown","source":"## Importing Datasets","metadata":{}},{"cell_type":"code","source":"train=pd.read_csv('/kaggle/input/mlp-term-2-2025-kaggle-assignment-3/train.csv')\ntest=pd.read_csv('/kaggle/input/mlp-term-2-2025-kaggle-assignment-3/test.csv')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-01T12:19:41.880438Z","iopub.execute_input":"2025-09-01T12:19:41.880688Z","iopub.status.idle":"2025-09-01T12:19:42.127414Z","shell.execute_reply.started":"2025-09-01T12:19:41.880670Z","shell.execute_reply":"2025-09-01T12:19:42.126783Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"train.columns = train.columns.str.strip()\ntest.columns = test.columns.str.strip()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-01T12:19:42.245245Z","iopub.execute_input":"2025-09-01T12:19:42.245479Z","iopub.status.idle":"2025-09-01T12:19:42.251052Z","shell.execute_reply.started":"2025-09-01T12:19:42.245461Z","shell.execute_reply":"2025-09-01T12:19:42.250434Z"}},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":"## Identifying data types of different columns","metadata":{}},{"cell_type":"code","source":"train.info()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-01T12:19:43.304656Z","iopub.execute_input":"2025-09-01T12:19:43.305189Z","iopub.status.idle":"2025-09-01T12:19:43.334507Z","shell.execute_reply.started":"2025-09-01T12:19:43.305162Z","shell.execute_reply":"2025-09-01T12:19:43.333733Z"}},"outputs":[{"name":"stdout","text":"<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 26500 entries, 0 to 26499\nData columns (total 10 columns):\n #   Column         Non-Null Count  Dtype  \n---  ------         --------------  -----  \n 0   id             26500 non-null  int64  \n 1   store_name     26500 non-null  object \n 2   category       26500 non-null  object \n 3   store_address  26500 non-null  object \n 4   latitude       25976 non-null  float64\n 5   longitude      25976 non-null  float64\n 6   rating_count   26500 non-null  object \n 7   review_time    26500 non-null  object \n 8   review         26500 non-null  object \n 9   rating         26500 non-null  int64  \ndtypes: float64(2), int64(2), object(6)\nmemory usage: 2.0+ MB\n","output_type":"stream"}],"execution_count":5},{"cell_type":"markdown","source":"## Handling duplicates","metadata":{}},{"cell_type":"code","source":"train = train.drop_duplicates()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-01T12:19:43.809751Z","iopub.execute_input":"2025-09-01T12:19:43.809966Z","iopub.status.idle":"2025-09-01T12:19:43.836751Z","shell.execute_reply.started":"2025-09-01T12:19:43.809949Z","shell.execute_reply":"2025-09-01T12:19:43.836273Z"}},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":"## Handling outliers","metadata":{}},{"cell_type":"code","source":"numerical_columns = [\"latitude\", \"longitude\"]\n\noutlier_indices_to_drop = set()\n\nfor column in numerical_columns:\n    Q1 = train[column].quantile(0.25)\n    Q3 = train[column].quantile(0.75)\n    IQR = Q3 - Q1\n\n    lower_bound = Q1 - 1.5 * IQR\n    upper_bound = Q3 + 1.5 * IQR\n\n    outliers_condition = (train[column] < lower_bound) | (train[column] > upper_bound)\n    outlier_indices_to_drop.update(train[outliers_condition].index.tolist())\n    \ntrain = train.drop(outlier_indices_to_drop)\nprint(f\"There are {len(outlier_indices_to_drop)} outliers in the train dataset\")\nprint(f\"Shape of the train_data after dropping outliers is {train.shape}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-01T12:19:44.164936Z","iopub.execute_input":"2025-09-01T12:19:44.165414Z","iopub.status.idle":"2025-09-01T12:19:44.192762Z","shell.execute_reply.started":"2025-09-01T12:19:44.165396Z","shell.execute_reply":"2025-09-01T12:19:44.192221Z"}},"outputs":[{"name":"stdout","text":"There are 0 outliers in the train dataset\nShape of the train_data after dropping outliers is (26500, 10)\n","output_type":"stream"}],"execution_count":7},{"cell_type":"markdown","source":"## Data Cleaning and Preparation","metadata":{}},{"cell_type":"code","source":"# Combine both datasets to create a consistent store_id mapping\ncombined = pd.concat([train[['latitude', 'longitude', 'store_address']].dropna().drop_duplicates(), \n                      test[['latitude', 'longitude']].dropna().drop_duplicates()], axis=0)\n\n\ncombined['store_id'] = combined.groupby(['latitude', 'longitude']).ngroup()\n\n# Create the store_id_map \nstore_id_map = combined.dropna().set_index('store_address')['store_id'].to_dict()\n\n# Prepare the training data with the new store_id \ntrain = train.merge(combined[['latitude', 'longitude', 'store_id']].drop_duplicates(), on=['latitude', 'longitude'], how='left')\ntrain = train.dropna(subset=['store_id'])\n\ntest = test.merge(combined[['latitude', 'longitude', 'store_id']].drop_duplicates(), on=['latitude', 'longitude'], how='left')\ntest = test.dropna(subset=['store_id'])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-01T12:19:44.489914Z","iopub.execute_input":"2025-09-01T12:19:44.490157Z","iopub.status.idle":"2025-09-01T12:19:44.525483Z","shell.execute_reply.started":"2025-09-01T12:19:44.490139Z","shell.execute_reply":"2025-09-01T12:19:44.524990Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"# Handle rating count\ntrain['rating_count'] = pd.to_numeric(train['rating_count'], errors='coerce').fillna(0)\ntest['rating_count'] = pd.to_numeric(test['rating_count'], errors='coerce').fillna(0)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-01T12:19:44.675260Z","iopub.execute_input":"2025-09-01T12:19:44.675471Z","iopub.status.idle":"2025-09-01T12:19:44.700613Z","shell.execute_reply.started":"2025-09-01T12:19:44.675455Z","shell.execute_reply":"2025-09-01T12:19:44.700044Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"train.drop(columns=['id', 'store_address', 'category', 'store_name', 'latitude', 'longitude'], inplace=True)\ntest.drop(columns=['id', 'store_address', 'category', 'store_name', 'latitude', 'longitude'], inplace=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-01T12:19:44.829438Z","iopub.execute_input":"2025-09-01T12:19:44.829788Z","iopub.status.idle":"2025-09-01T12:19:44.835723Z","shell.execute_reply.started":"2025-09-01T12:19:44.829772Z","shell.execute_reply":"2025-09-01T12:19:44.835113Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"train.isnull().sum()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-01T12:19:50.026388Z","iopub.execute_input":"2025-09-01T12:19:50.026633Z","iopub.status.idle":"2025-09-01T12:19:50.037091Z","shell.execute_reply.started":"2025-09-01T12:19:50.026616Z","shell.execute_reply":"2025-09-01T12:19:50.036545Z"}},"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"rating_count    0\nreview_time     0\nreview          0\nrating          0\nstore_id        0\ndtype: int64"},"metadata":{}}],"execution_count":11},{"cell_type":"code","source":"test.isnull().sum()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-01T12:19:50.204553Z","iopub.execute_input":"2025-09-01T12:19:50.204727Z","iopub.status.idle":"2025-09-01T12:19:50.211527Z","shell.execute_reply.started":"2025-09-01T12:19:50.204713Z","shell.execute_reply":"2025-09-01T12:19:50.210851Z"}},"outputs":[{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"rating_count    0\nreview_time     0\nreview          0\nstore_id        0\ndtype: int64"},"metadata":{}}],"execution_count":12},{"cell_type":"markdown","source":"## Preprocessing Pipeline","metadata":{}},{"cell_type":"code","source":"# !pip install transformers torch","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-01T12:19:50.599617Z","iopub.execute_input":"2025-09-01T12:19:50.600082Z","iopub.status.idle":"2025-09-01T12:19:50.602723Z","shell.execute_reply.started":"2025-09-01T12:19:50.600045Z","shell.execute_reply":"2025-09-01T12:19:50.601971Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"class TextReviewTimeTransformer(BaseEstimator, TransformerMixin):\n    def __init__(self):\n        pass\n\n    def fit(self, X, y=None):\n        return self\n\n    def transform(self, X):\n        X_copy = X.copy()\n        \n        # New function to parse text-based review times\n        def parse_review_time(text):\n            today = pd.Timestamp.today()\n            match = re.match(r\"^(a|\\d+)\\s+(year|month|week|day|hour)s?\\s+ago$\", str(text).strip())\n            if not match:\n                return np.nan\n            value_str, unit = match.groups()\n            value = 1 if value_str == \"a\" else int(value_str)\n            \n            if unit == \"year\":\n                delta = timedelta(days=365 * value)\n            elif unit == \"month\":\n                delta = timedelta(days=30 * value)\n            elif unit == \"week\":\n                delta = timedelta(weeks=value)\n            elif unit == \"day\":\n                delta = timedelta(days=value)\n            elif unit == \"hour\":\n                delta = timedelta(hours=value)\n            else:\n                return np.nan\n            return today - delta\n\n        # Apply the parsing and calculate days ago\n        review_date = X_copy['review_time'].apply(parse_review_time)\n        days_ago = (pd.Timestamp.today() - review_date).dt.days\n        days_ago = days_ago.fillna(0) \n        \n        return pd.DataFrame(days_ago, columns=['review_time'])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-01T12:19:50.809613Z","iopub.execute_input":"2025-09-01T12:19:50.809790Z","iopub.status.idle":"2025-09-01T12:19:50.816096Z","shell.execute_reply.started":"2025-09-01T12:19:50.809776Z","shell.execute_reply":"2025-09-01T12:19:50.815539Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModel\nimport torch\nimport numpy as np\nfrom sklearn.base import BaseEstimator, TransformerMixin\n\nclass BertVectorizer(BaseEstimator, TransformerMixin):\n    def __init__(self, model_name=\"nlptown/bert-base-multilingual-uncased-sentiment\", max_length=128, device=None):\n        self.model_name = model_name\n        self.max_length = max_length\n        self.device = device if device else (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n        self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)\n        self.model = AutoModel.from_pretrained(self.model_name)\n        self.model.to(self.device)\n        self.model.eval()\n    \n    def fit(self, X, y=None):\n        return self\n    \n    def transform(self, X):\n        # Encode texts batch-wise for performance\n        texts = X.iloc[:, 0].tolist()\n        batch_size = 16\n        vectors = []\n        with torch.no_grad():\n            for i in range(0, len(texts), batch_size):\n                batch_texts = texts[i:i+batch_size]\n                encoding = self.tokenizer(batch_texts, \n                                          padding=True, \n                                          truncation=True, \n                                          max_length=self.max_length, \n                                          return_tensors=\"pt\")\n                input_ids = encoding['input_ids'].to(self.device)\n                attention_mask = encoding['attention_mask'].to(self.device)\n                \n                outputs = self.model(input_ids, attention_mask=attention_mask)\n                # Use the CLS token representation as sentence embedding\n                cls_embeddings = outputs.last_hidden_state[:, 0, :].cpu().numpy()\n                vectors.append(cls_embeddings)\n        return np.vstack(vectors)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-01T12:19:51.029998Z","iopub.execute_input":"2025-09-01T12:19:51.030321Z","iopub.status.idle":"2025-09-01T12:19:57.670381Z","shell.execute_reply.started":"2025-09-01T12:19:51.030294Z","shell.execute_reply":"2025-09-01T12:19:57.669816Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"X_train, Y_train = train.drop([\"rating\"], axis=1), train[[\"rating\"]]\n\ntime_transformer = Pipeline(steps=[\n    ('time', TextReviewTimeTransformer()),\n    ('scaler', StandardScaler())\n])\n\npreprocessor  = ColumnTransformer(\n    transformers=[\n        ('time', time_transformer, ['review_time']),\n        ('bert', BertVectorizer(), ['review']),\n        ('numeric_features', StandardScaler(), ['rating_count', 'store_id']),\n    ],\n    remainder='passthrough'\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-01T12:19:57.671504Z","iopub.execute_input":"2025-09-01T12:19:57.671924Z","iopub.status.idle":"2025-09-01T12:20:22.311503Z","shell.execute_reply.started":"2025-09-01T12:19:57.671907Z","shell.execute_reply":"2025-09-01T12:20:22.310851Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/39.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5e904435ad5d429e8fb4b4939bfeeba4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/953 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d4c8f5251bdf462ba6c919d2e9c16754"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"25cb449621664819b6d0ed84eb07022a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fdc9e3b102ea4d01bb33ab383e638ced"}},"metadata":{}},{"name":"stderr","text":"2025-09-01 12:20:08.507678: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1756729208.690539      36 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1756729208.741229      36 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/669M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fd89b8616ebb4844a447e71abc916237"}},"metadata":{}}],"execution_count":16},{"cell_type":"markdown","source":"## Model Training","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import StratifiedKFold\nfrom sklearn.ensemble import StackingClassifier\nfrom sklearn.calibration import CalibratedClassifierCV\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom xgboost import XGBClassifier\n\nrf_base = RandomForestClassifier(\n    n_estimators=600,\n    max_depth=None,\n    max_features='sqrt',\n    min_samples_split=2,\n    min_samples_leaf=1,\n    n_jobs=-1,\n    random_state=42\n)\n\nxgb_base = XGBClassifier(\n    objective='multi:softprob',\n    num_class=5,\n    n_estimators=500,\n    learning_rate=0.05,\n    max_depth=6,\n    subsample=0.9,\n    colsample_bytree=0.8,\n    eval_metric='mlogloss',\n    tree_method='hist',          \n    random_state=42\n)\n\nmlp_base = MLPClassifier(\n    hidden_layer_sizes=(256, 128),\n    activation='relu',\n    alpha=1e-4,\n    learning_rate='adaptive',\n    max_iter=300,\n    random_state=42\n)\n\nrf_cal = CalibratedClassifierCV(rf_base, method='sigmoid', cv=3)\nxgb_cal = xgb_base  \nmlp_cal = CalibratedClassifierCV(mlp_base, method='sigmoid', cv=3)\n\nestimators = [\n    ('rf', rf_cal),\n    ('xgb', xgb_cal),\n    ('mlp', mlp_cal),\n]\n\nmeta_xgb = XGBClassifier(\n    objective='multi:softprob',\n    num_class=5,\n    n_estimators=300,\n    learning_rate=0.05,\n    max_depth=4,\n    subsample=0.9,\n    colsample_bytree=0.8,\n    eval_metric='mlogloss',\n    tree_method='hist',\n    random_state=42\n)\n\ncv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n\nstack_clf = StackingClassifier(\n    estimators=estimators,\n    final_estimator=meta_xgb,     \n    cv=5,\n    passthrough=True,             \n    n_jobs=-1\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-01T12:20:22.312228Z","iopub.execute_input":"2025-09-01T12:20:22.312916Z","iopub.status.idle":"2025-09-01T12:20:22.654344Z","shell.execute_reply.started":"2025-09-01T12:20:22.312890Z","shell.execute_reply":"2025-09-01T12:20:22.653751Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"pipeline = Pipeline(steps=[\n    ('preprocessor', preprocessor),\n    ('classifier', stack_clf)\n])\n\npipeline.fit(X_train, Y_train)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-01T12:20:22.655431Z","iopub.execute_input":"2025-09-01T12:20:22.655698Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Pickle the pipeline and store ID map","metadata":{}},{"cell_type":"code","source":"with open('final_pipeline.pkl', 'wb') as f:\n    pickle.dump(pipeline, f)\n\nwith open('store_map.pkl', 'wb') as f:\n    pickle.dump(store_id_map, f)\n\nprint(\"Model and new store ID map have been pickled successfully!\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Prediction","metadata":{}},{"cell_type":"code","source":"predictions = pipeline.predict(test)\n\nsubmission_df = pd.DataFrame({'id': range(len(predictions)), 'rating': predictions})\nsubmission_df.to_csv('submission.csv', index=False)\nprint(\"âœ… Submitted csv file successfully!\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}